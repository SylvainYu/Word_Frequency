{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "n = 3  # 3-gram\n",
    "\n",
    "data_path       = './Records_data.txt'          # 存放预处理后监督记录数据\n",
    "wordtable_path  = './wordtable.txt'  # 词表\n",
    "stopwords_path  = './CSDN_stopwords.txt' # 停止词表\n",
    "testset_path    = './questions.txt'       # 测试集\n",
    "prediction_path = './predictions.txt'   # 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbc in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a812e2f1f8d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 遍历所有预处理过的新闻文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'<BOS>'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'<EOS>'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# 列表，形如：['<BOS>', '显得', '十分', '明亮', '<EOS>']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mngrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 一个句子中n-gram元组的列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbc in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "ngrams_list = []  # n元组（分子）\n",
    "prefix_list = []  # n-1元组（分母）\n",
    "\n",
    "# 遍历所有预处理过的新闻文件\n",
    "with open(data_path') as f:\n",
    "    for line in f:\n",
    "        sentence = ['<BOS>'] + line.split() + ['<EOS>']  # 列表，形如：['<BOS>', '显得', '十分', '明亮', '<EOS>']\n",
    "        ngrams = list(zip(*[sentence[i:] for i in range(n)]))   # 一个句子中n-gram元组的列表\n",
    "        prefix = list(zip(*[sentence[i:] for i in range(n-1)])) # 历史前缀元组的列表\n",
    "        ngrams_list += ngrams\n",
    "        prefix_list += prefix\n",
    "\n",
    "ngrams_counter = Counter(ngrams_list)\n",
    "prefix_counter = Counter(prefix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []  # 词表中的全部词\n",
    "with open(wordtable_path, encoding='utf-8') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        all_words.append(line.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止词\n",
    "with open(stopwords_path, encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = set(map(lambda x:x.strip(), stopwords))  # 去除末尾换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(sentence):\n",
    "    \"\"\"\n",
    "    计算一个句子的概率。\n",
    "    Params:\n",
    "        sentence: 由词构成的列表表示的句子。\n",
    "    Returns:\n",
    "        句子的概率。\n",
    "    \"\"\"\n",
    "    prob = 1  # 初始化句子概率\n",
    "    ngrams = list(zip(*[sentence[i:] for i in range(n)]))   # 将句子处理成n-gram的列表\n",
    "    for ngram in ngrams:\n",
    "        # 累乘每个n-gram的概率，并使用加一法进行数据平滑\n",
    "        prob *= (1 + ngrams_counter[ngram]) / (len(prefix_counter) + prefix_counter[(ngram[0], ngram[1])])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pre_sentence, post_sentence, all_words, cand_num=1):\n",
    "    \"\"\"\n",
    "    根据历史进行一个词的预测。\n",
    "    Params:\n",
    "        pre_sentence: 待预测词之前部分句子的分词结果构成的列表。\n",
    "        post_sentence: 待预测词之后部分句子的分词结果构成的列表。\n",
    "        all_words: 所有候选词构成的列表。\n",
    "        cand_num: 候选词数，默认为1。\n",
    "    Returns:\n",
    "        一个含有cand_num个元素的列表，表示预测的词，概率由大到小排序；\n",
    "        如果预测失败，返回None。\n",
    "    \"\"\"\n",
    "    word_prob = []  # 候选词及其概率构成的元组的列表\n",
    "    for word in all_words:\n",
    "        # 实际上不需要算整个句子的概率，只需要算待预测词附近的概率即可，因为句子其他部分的概率不受待预测词影响\n",
    "        test_sentence = pre_sentence[-(n-1):] + [word] + post_sentence[:(n-1)]  # 待预测词及其前后各n-1个词的列表\n",
    "        word_prob.append( (word, probability(test_sentence)) )                  # (词, 概率)元组构成的列表\n",
    "\n",
    "    return sorted(word_prob, key=lambda tup: tup[1], reverse=True)[:cand_num]  # 按概率降序排序并取前cand_num个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载测试集标签（答案）\n",
    "with open('./questions.txt', encoding='utf-8') as f:\n",
    "    answers = [answer.strip() for answer in f]  # 答案构成的列表\n",
    "    \n",
    "prediction_file = open(prediction_path, 'w', encoding='utf-8')  # 存放预测结果\n",
    "\n",
    "# 开始测试\n",
    "correct_count = 0  # 预测正确的数量\n",
    "\n",
    "with open('questions.txt', encoding='utf-8') as f:\n",
    "    questions = f.readlines()  # 测试集规模\n",
    "    total_count = len(questions)\n",
    "    for i, question in enumerate(questions):\n",
    "        question = question.strip()\n",
    "        pre_mask = question[:question.index('[MASK]')]     # 待预测词的历史\n",
    "        post_mask = question[question.index('[MASK]')+6:]  # 待预测词后的剩余部分\n",
    "        \n",
    "        pre_sentence = jieba.cut(pre_mask.replace('，', ' '))  # 分词\n",
    "        post_sentence = jieba.cut(post_mask.replace('，', ' '))  # 分词\n",
    "        pre_sentence = [word.strip() for word in pre_sentence if word.strip() and word not in stopwords]  # 去除停止词、空串\n",
    "        post_sentence = [word.strip() for word in post_sentence if word.strip() and word not in stopwords]  # 去除停止词、空串\n",
    "\n",
    "        predict_cand = predict(pre_sentence, post_sentence, all_words)  # 预测一个概率最大的词\n",
    "        prediction_file.write(' '.join([w[0] for w in predict_cand]) + '\\n')  # 将预测结果写入文件\n",
    "\n",
    "        # 遍历多个预测结果\n",
    "        for j, p in enumerate(predict_cand):\n",
    "            if p[0] == answers[i]:\n",
    "                print(i, '{} [{}] {}'.format(pre_mask, p[0], post_mask))\n",
    "                correct_count += 1\n",
    "                break\n",
    "                    \n",
    "prediction_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('准确率：{}/{}'.format(correct_count, total_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
