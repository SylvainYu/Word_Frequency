{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Records Data, and then Clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _*_ coding:utf-8 _*_\n",
    "# n-gram\n",
    "# 出自CSDN博客\"n-gram python实现（基于sklearn）_Sinsa_SI_20170807\"，20200715见\n",
    "# 出自CSDN博客\"Python---n-gram实现_GeekZW_20191207\",20200715见\n",
    "# 出自JED伪极客\"用Python实现n-gram语言模型进行新闻文本内容预测_Jed_20191201\"，20200715见，后发现本文判断关联词应用较为实际\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "df = pd.read_csv('~\\Downloads\\AllRecords200518_Districts.csv') # DataFrame结构，提取年份、月份、日期和属地、姓名，分别统计\n",
    "#df = pd.read_csv(r'C:\\users\\administrator\\documents\\github\\word_frequency\\AllRecords200518_Districts.csv') # DataFrame结构，提取年份、月份、日期和属地、姓名，分别统计\n",
    "\n",
    "#df.loc[df['姓名'] == '邬嘉荪', ['监督记录']].to_csv()\n",
    "somebody = df.loc[df['姓名'] == '邬嘉荪', ['监督记录']].to_csv()\n",
    "# then cut the data into sentences, via \"。？！\",etc, to files.\n",
    "# after that cut sentences to words, store into files.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=============================================================================\n",
    "#data = [\"他用报话机向上级呼喊：“为了祖国，为了胜利，向我开炮！向我开炮！\", \"记者：你怎么会说出那番话？\", \"韦昌进：我只是觉得，对准我自己打，才有可能把上了我哨位的这些敌人打死，或者打下去。\"]\n",
    "\n",
    "# data = [\" \".join(jieba.lcut(e)) for e in data] #分词，并用空格连接\n",
    "\n",
    "# vec = CountVectorizer(min_df=1, ngram_range=(1,2))\n",
    "# ngram_range=(1,1)标识unigram, ngram_range=(2,2)表示bigram, ngram_range=(3,3)表示trigram\n",
    "#=============================================================================\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordfrequency_jieba as wf\n",
    "import stopwords_cn as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.747 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# cleanwords: 特定人监督记录用户词典辨识，并去停止词结果；\n",
    "# cleanwords中使用'/'分隔词汇，这由drop_stopwords()定义\n",
    "cleanwords = sw.drop_stopwords(df.loc[df['姓名'] == '邬嘉荪', ['监督记录']].to_csv()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
